{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e097db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\Yash Verma\\Desktop\\College\\5th Sem\\Data Science\\Project\\PG\\notebooks\n",
      "Outputs directory: C:\\Users\\Yash Verma\\Desktop\\College\\5th Sem\\Data Science\\Project\\PG\\outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "OUT = Path(\"../outputs\")\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Working directory:\", Path.cwd())\n",
    "print(\"Outputs directory:\", OUT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b037c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dir: C:\\Users\\Yash Verma\\Desktop\\College\\5th Sem\\Data Science\\Project\\PG\\data\n",
      "Cleaned file path: ..\\outputs\\cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Configuration: change if your CSV folder is different\n",
    "DATA_DIR = Path(\"../data/\")   # folder containing the 8 CSVs\n",
    "CLEANED_PATH = OUT / \"cleaned_data.csv\"\n",
    "force_merge = False   # set True to re-merge raw CSVs even if cleaned_data.csv exists\n",
    "\n",
    "print(\"Data dir:\", DATA_DIR.resolve() if DATA_DIR.exists() else \"NOT FOUND\")\n",
    "print(\"Cleaned file path:\", CLEANED_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fa91023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files. Merging...\n",
      "  [1/8] Reading Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "  [2/8] Reading Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "  [3/8] Reading Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "  [4/8] Reading Monday-WorkingHours.pcap_ISCX.csv\n",
      "  [5/8] Reading Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "  [6/8] Reading Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "  [7/8] Reading Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "  [8/8] Reading Wednesday-workingHours.pcap_ISCX.csv\n",
      "Merged shape: (2830743, 79)\n",
      "Merged file saved to: ..\\outputs\\cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Load or merge CSVs\n",
    "if CLEANED_PATH.exists() and not force_merge:\n",
    "    print(\"Loading existing cleaned file:\", CLEANED_PATH)\n",
    "    df = pd.read_csv(CLEANED_PATH, low_memory=False)\n",
    "else:\n",
    "    # find CSVs\n",
    "    csv_files = sorted(list(DATA_DIR.glob(\"*.csv\")))\n",
    "    if len(csv_files) == 0:\n",
    "        raise FileNotFoundError(f\"No CSVs found in {DATA_DIR}. Place your CIC-IDS-2017 CSVs there.\")\n",
    "    print(f\"Found {len(csv_files)} CSV files. Merging...\")\n",
    "\n",
    "    parts = []\n",
    "    for i, p in enumerate(csv_files, 1):\n",
    "        print(f\"  [{i}/{len(csv_files)}] Reading {p.name}\")\n",
    "        parts.append(pd.read_csv(p, low_memory=False))\n",
    "    df = pd.concat(parts, axis=0, ignore_index=True, sort=False)\n",
    "    print(\"Merged shape:\", df.shape)\n",
    "\n",
    "    # write initial merged file (optional)\n",
    "    try:\n",
    "        df.to_csv(CLEANED_PATH, index=False)\n",
    "        print(\"Merged file saved to:\", CLEANED_PATH)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: could not save merged file:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4df38b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 79\n",
      "Sample columns: ['Destination Port', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets', 'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean', 'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min']\n",
      "Saved columns_list.csv to outputs/\n"
     ]
    }
   ],
   "source": [
    "# Normalize column names (strip and collapse multiple spaces)\n",
    "df.columns = [(\" \".join(str(c).split())).strip() for c in df.columns]\n",
    "\n",
    "print(\"Number of columns:\", len(df.columns))\n",
    "print(\"Sample columns:\", df.columns.tolist()[:25])\n",
    "# Save column list for inspection\n",
    "pd.Series(df.columns.tolist(), name=\"columns\").to_csv(OUT / \"columns_list.csv\", index=False)\n",
    "print(\"Saved columns_list.csv to outputs/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "324a93d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top columns by missing fraction (showing 20):\n",
      "Flow Bytes/s                   0.00048\n",
      "Flow Duration                  0.00000\n",
      "Destination Port               0.00000\n",
      "Total Backward Packets         0.00000\n",
      "Total Length of Fwd Packets    0.00000\n",
      "Total Length of Bwd Packets    0.00000\n",
      "Total Fwd Packets              0.00000\n",
      "Fwd Packet Length Max          0.00000\n",
      "Fwd Packet Length Min          0.00000\n",
      "Fwd Packet Length Std          0.00000\n",
      "Fwd Packet Length Mean         0.00000\n",
      "Bwd Packet Length Max          0.00000\n",
      "Bwd Packet Length Min          0.00000\n",
      "Bwd Packet Length Mean         0.00000\n",
      "Bwd Packet Length Std          0.00000\n",
      "Flow Packets/s                 0.00000\n",
      "Flow IAT Mean                  0.00000\n",
      "Flow IAT Std                   0.00000\n",
      "Flow IAT Max                   0.00000\n",
      "Flow IAT Min                   0.00000\n",
      "dtype: float64\n",
      "Shape after dropping high-missing columns: (2830743, 79)\n"
     ]
    }
   ],
   "source": [
    "# Missing value summary\n",
    "missing_frac = df.isna().mean().sort_values(ascending=False)\n",
    "top_missing = missing_frac.head(20)\n",
    "print(\"Top columns by missing fraction (showing 20):\")\n",
    "print(top_missing)\n",
    "\n",
    "# Drop columns with >30% missing (tunable)\n",
    "drop_threshold = 0.30\n",
    "cols_to_drop = missing_frac[missing_frac > drop_threshold].index.tolist()\n",
    "if cols_to_drop:\n",
    "    print(f\"Dropping {len(cols_to_drop)} columns with >{int(drop_threshold*100)}% missing.\")\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(\"Shape after dropping high-missing columns:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c792bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coercing to numeric (columns found): ['Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Flow Bytes/s', 'Flow Packets/s', 'Average Packet Size', 'Fwd Packet Length Mean', 'Bwd Packet Length Mean']\n"
     ]
    }
   ],
   "source": [
    "# Replace infinite values with NaN across numeric dtypes\n",
    "numcols_before = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df[numcols_before] = df[numcols_before].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Explicit numeric columns to coerce (edit if your dataset names differ)\n",
    "numeric_cols = [\n",
    "    'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets',\n",
    "    'Flow Bytes/s', 'Flow Packets/s', 'Average Packet Size', 'Fwd Packet Length Mean',\n",
    "    'Bwd Packet Length Mean'\n",
    "]\n",
    "# Keep only those that actually exist in df\n",
    "numeric_cols_present = [c for c in numeric_cols if c in df.columns]\n",
    "\n",
    "print(\"Coercing to numeric (columns found):\", numeric_cols_present)\n",
    "for c in numeric_cols_present:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e538d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 115 negative Flow Duration values to NaN\n",
      "Numeric columns count: 78\n",
      "Imputed numeric NaNs with median.\n"
     ]
    }
   ],
   "source": [
    "# Example domain fixes: negative durations -> NaN\n",
    "if 'Flow Duration' in df.columns:\n",
    "    neg_dur = (df['Flow Duration'] < 0).sum()\n",
    "    if neg_dur:\n",
    "        print(f\"Setting {neg_dur} negative Flow Duration values to NaN\")\n",
    "        df.loc[df['Flow Duration'] < 0, 'Flow Duration'] = np.nan\n",
    "\n",
    "# Impute numeric NaNs with median (simple and robust)\n",
    "numeric_cols_all = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numeric columns count:\", len(numeric_cols_all))\n",
    "df[numeric_cols_all] = df[numeric_cols_all].fillna(df[numeric_cols_all].median())\n",
    "print(\"Imputed numeric NaNs with median.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b8a867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 308381 duplicate rows\n",
      "Shape after dedup: (2522362, 79)\n"
     ]
    }
   ],
   "source": [
    "# Remove exact duplicate rows\n",
    "dup_count = df.duplicated().sum()\n",
    "if dup_count:\n",
    "    print(f\"Dropping {dup_count} duplicate rows\")\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "else:\n",
    "    df = df.reset_index(drop=True)\n",
    "print(\"Shape after dedup:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ccf4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label column: Label\n",
      "Saved class_balance.csv (BENIGN vs ATTACK counts)\n"
     ]
    }
   ],
   "source": [
    "# Detect label column (expected 'Label'); try common names otherwise\n",
    "possible_label_cols = [c for c in df.columns if c.lower().strip() in ('label','class','attack','attack_label')]\n",
    "label_col = possible_label_cols[0] if possible_label_cols else None\n",
    "\n",
    "if not label_col:\n",
    "    # fallback: heuristic: string column with 'BENIGN' samples\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            s = df[c].dropna().astype(str).str.upper().head(200).tolist()\n",
    "            if any('BENIGN' in v for v in s):\n",
    "                label_col = c\n",
    "                break\n",
    "\n",
    "if not label_col:\n",
    "    raise RuntimeError(\"Label column not found automatically. Edit notebook and set label_col manually.\")\n",
    "\n",
    "print(\"Using label column:\", label_col)\n",
    "df[label_col] = df[label_col].astype(str).str.strip().str.upper()\n",
    "df['Attack_Binary'] = df[label_col].apply(lambda x: 'BENIGN' if 'BENIGN' in x else 'ATTACK')\n",
    "\n",
    "# Save class balance\n",
    "df['Attack_Binary'].value_counts().to_csv(OUT / \"class_balance.csv\", header=['count'])\n",
    "print(\"Saved class_balance.csv (BENIGN vs ATTACK counts)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4061b0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned dataset to: ..\\outputs\\cleaned_data.csv\n",
      "Saved descriptive_summary.csv\n",
      "Saved sample_head.csv\n"
     ]
    }
   ],
   "source": [
    "# Final save\n",
    "cleaned_path = OUT / \"cleaned_data.csv\"\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "print(\"Saved cleaned dataset to:\", cleaned_path)\n",
    "\n",
    "# Basic descriptive summary\n",
    "desc = df.describe(include='all').T\n",
    "desc.to_csv(OUT / \"descriptive_summary.csv\")\n",
    "print(\"Saved descriptive_summary.csv\")\n",
    "\n",
    "# Sample head for quick inspection\n",
    "df.head(5).to_csv(OUT / \"sample_head.csv\", index=False)\n",
    "print(\"Saved sample_head.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b624583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows,Cols: (2522362, 80)\n",
      "Top 10 columns and types:\n",
      "Destination Port                 int64\n",
      "Flow Duration                  float64\n",
      "Total Fwd Packets                int64\n",
      "Total Backward Packets           int64\n",
      "Total Length of Fwd Packets      int64\n",
      "Total Length of Bwd Packets      int64\n",
      "Fwd Packet Length Max            int64\n",
      "Fwd Packet Length Min            int64\n",
      "Fwd Packet Length Mean         float64\n",
      "Fwd Packet Length Std          float64\n",
      "dtype: object\n",
      "Any NaNs remaining in numeric columns (counts):\n",
      "Destination Port               0\n",
      "Flow Duration                  0\n",
      "Total Fwd Packets              0\n",
      "Total Backward Packets         0\n",
      "Total Length of Fwd Packets    0\n",
      "Total Length of Bwd Packets    0\n",
      "Fwd Packet Length Max          0\n",
      "Fwd Packet Length Min          0\n",
      "Fwd Packet Length Mean         0\n",
      "Fwd Packet Length Std          0\n",
      "Bwd Packet Length Max          0\n",
      "Bwd Packet Length Min          0\n",
      "Bwd Packet Length Mean         0\n",
      "Bwd Packet Length Std          0\n",
      "Flow Bytes/s                   0\n",
      "dtype: int64\n",
      "First 3 unique labels: ['BENIGN' 'DDOS' 'PORTSCAN' 'BOT' 'INFILTRATION'\n",
      " 'WEB ATTACK � BRUTE FORCE' 'WEB ATTACK � XSS'\n",
      " 'WEB ATTACK � SQL INJECTION' 'FTP-PATATOR' 'SSH-PATATOR']\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity checks to print in notebook\n",
    "print(\"Rows,Cols:\", df.shape)\n",
    "print(\"Top 10 columns and types:\")\n",
    "print(df.dtypes.head(10))\n",
    "print(\"Any NaNs remaining in numeric columns (counts):\")\n",
    "print(df.select_dtypes(include=[np.number]).isna().sum().sort_values(ascending=False).head(15))\n",
    "print(\"First 10 unique labels:\", df[label_col].unique()[:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
